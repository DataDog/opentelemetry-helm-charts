receivers:
  prometheus:
    config:
      scrape_configs:
        - job_name: "otelcol"
          scrape_interval: 10s
          static_configs:
            - targets: ["0.0.0.0:8888"]
          metric_relabel_configs:
            - source_labels: [__name__]
              regex: ".*grpc_io.*"
              action: drop
  k8s_cluster:
    collection_interval: 10s
  hostmetrics:
    collection_interval: 10s
    scrapers:
      paging:
        metrics:
          system.paging.utilization:
            enabled: true
          system.paging.usage:
            enabled: true
      cpu:
        metrics:
          system.cpu.utilization:
            enabled: true
      disk:
      filesystem:
        metrics:
          system.filesystem.utilization:
            enabled: true
      load:
      memory:
        metrics:
          system.memory.utilization:
            enabled: true
      network:
        metrics:
          system.network.io.receive:
            enabled: true
          system.network.io.transmit:
            enabled: true
      processes:
  otlp:
    protocols:
      grpc:
      http:

  kubeletstats:
    auth_type: serviceAccount
    collection_interval: 60s
    insecure_skip_verify: true
    k8s_api_config:
      auth_type: serviceAccount
    metric_groups:
      - node
      - pod
      - container

  filelog:
    include:
      - /var/log/pods/**/*.log
    start_at: end
    poll_interval: 500ms
    operators:
      # Support docker and containerd runtimes, which have different
      # logging formats.
      - type: router
        routes:
          - expr: 'body matches "^[^\\s]+ \\w+ .*"'
            output: containerd_parser
        default: docker_parser
      # The raw message looks like this:
      # {"log":"I0618 14:30:29.641678       1 logs.go:59] http: TLS handshake error from 192.168.49.2:56222: EOF\n","stream":"stderr","time":"2022-06-18T14:30:29.641732743Z"}
      - type: json_parser
        id: docker_parser
        timestamp:
          parse_from: attributes.time
          layout: "%Y-%m-%dT%H:%M:%S.%sZ"
        output: log-to-body
      # The raw message looks like this:
      # 2022-06-18T16:52:59.639114537Z stdout F {"message":"registered Stackdriver tracing","severity":"info","timestamp":"2022-06-18T16:52:59.639034532Z"}
      - id: containerd_parser
        type: regex_parser
        regex: '^(?P<time>[^\s]+) (?P<stream>\w+) (?P<partial>\w)?(?P<log>.*)'
      - type: recombine
        source_identifier: attributes["log.file.name"]
        combine_field: attributes.log
        is_last_entry: "attributes.partial == 'F'"
      - type: remove
        field: attributes.partial
      - id: time_parser_router
        type: router
        routes:
          # Containerd can have a couple timestamp formats depending if the node has local time set
          - output: local_containerd_timestamp_parser
            expr: 'attributes.time != nil and attributes.time matches "^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{3,9}[\\+-]\\d{2}:\\d{2}"'
          - output: utc_containerd_timestamp_parser
            expr: 'attributes.time != nil and attributes.time matches "^\\d{4}-\\d{2}-\\d{2}T\\d{2}:\\d{2}:\\d{2}\\.\\d{3,9}Z"'
      - type: time_parser
        id: local_containerd_timestamp_parser
        parse_from: attributes.time
        layout: "%Y-%m-%dT%H:%M:%S.%s%j"
        output: log-to-body
      - type: time_parser
        id: utc_containerd_timestamp_parser
        parse_from: attributes.time
        layout: "%Y-%m-%dT%H:%M:%S.%sZ"
        output: log-to-body
      # The raw body does not contain anything useful considering timestamp has been promotoed to
      # the log entries timestamp, therefore we move attributes.log (the actual container log message)
      # to body.
      - type: move
        id: log-to-body
        from: attributes.log
        to: body
      # Detect pod, namespace, and container names from the file name.
      - type: regex_parser
        regex: '^(?P<pod>[a-z0-9]([-a-z0-9]*[a-z0-9])?(\.[a-z0-9]([-a-z0-9]*[a-z0-9])?)*)_(?P<namespace>[^_]+)_(?P<container>.+)-'
        parse_from: attributes["log.file.name"]
        cache:
          size: 500
      # Semantic conventions for k8s
      # https://github.com/open-telemetry/opentelemetry-specification/blob/main/specification/resource/semantic_conventions/k8s.md#kubernetes
      - type: move
        from: attributes.pod
        to: resource["k8s.pod.name"]
      - type: move
        from: attributes.namespace
        to: resource["k8s.namespace.name"]
      - type: move
        from: attributes.container
        to: resource["k8s.container.name"]

exporters:
  datadog:
    api:
      key: "$DD_API_KEY"

processors:
  resourcedetection:
    # ensures host.name and other important resource tags
    # get picked up
    detectors: [env, gcp, ecs, ec2, azure, system]
    timeout: 5s
    override: false
  # adds various tags related to k8s
  k8sattributes:
  batch:
    send_batch_max_size: 1000
    send_batch_size: 100
    timeout: 10s

service:
  pipelines:
    metrics:
      receivers: [otlp, k8s_cluster, hostmetrics, prometheus]
      processors: [resourcedetection, k8sattributes, metricstransform, batch]
      exporters: [datadog]
    traces:
      receivers: [otlp]
      processors: [resourcedetection, k8sattributes, batch]
      exporters: [datadog]
